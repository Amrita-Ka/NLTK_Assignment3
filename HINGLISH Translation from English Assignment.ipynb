{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download()       #punkt select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement python (from versions: none)\n",
      "ERROR: No matching distribution found for python\n"
     ]
    }
   ],
   "source": [
    "pip install python 3.n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\amrit\\anaconda3\\lib\\site-packages (0.1.99)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\")\n",
    "tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spot_nouns_verbs(en_article):\n",
    "    words = word_tokenize(en_article)\n",
    "    tagged_words = pos_tag(words)\n",
    "    auxiliary_verbs = ['am', 'is', 'are', 'was', 'were', 'has', 'had']\n",
    "    nouns = [words for words, pos in tagged_words if pos.startswith('NN')]\n",
    "    # avoids auxiliary verbs\n",
    "    verbs = [word for word, pos in tagged_words if pos.startswith('VB')and word not in auxiliary_verbs] \n",
    "    # to bring verbs to its base form\n",
    "    verbs = [lemmatizer.lemmatize(verb, pos='v') for verb in verbs]\n",
    "    # dictionary to store english and HINGLISH nouns\n",
    "    translated_words = {'feedback':'प्रतिक्रिया', 'definitely':'निश्चित', 'section':'खण्ड'} \n",
    "    for noun in nouns:        \n",
    "            hi_noun = hi_translation(noun)                \n",
    "            translated_words[noun] = hi_noun    \n",
    "    for verb in verbs:\n",
    "        hi_verb = hi_translation(verb)\n",
    "        modified_value = hi_verb.split(' ', 1)[0]  # Split by the first space and keep the first part\n",
    "        translated_words[verb] = modified_value    \n",
    "    return translated_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English to Hindi Translation\n",
    "def hi_translation(en_article):\n",
    "    inputs = tokenizer.encode(en_article, return_tensors=\"pt\")\n",
    "    translated_id = model.generate(inputs, max_length=150, num_return_sequences=1, num_beams=4)\n",
    "    translated_output=tokenizer.decode(translated_id[0], skip_special_tokens=True)  \n",
    "    translated_output = translated_output.replace('\\u200d', '') # handle ZWJ characters\n",
    "    return translated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switching Hindi nouns to English nouns to keep certain words in English\n",
    "def noun_switch(nouns, hinglish_text):\n",
    "    for key, value in nouns.items():\n",
    "        matches = re.findall(r'\\b' + re.escape(value) + r'\\b', hinglish_text)    \n",
    "        for match in matches:\n",
    "            hinglish_text = hinglish_text.replace(match, key)\n",
    "    return hinglish_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\amrit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT : Definitely share your feedback in the comment section.\n",
      "comment खण्ड में अपनी प्रतिक्रिया को definitely ही share करें ।\n"
     ]
    }
   ],
   "source": [
    "# SENTENCE 1\n",
    "en_article = input(\"INPUT : \")\n",
    "nouns = spot_nouns_verbs(en_article)\n",
    "hi_text = hi_translation(en_article)\n",
    "# Loop through the dictionary and perform replacements\n",
    "for eng_word, hin_word in nouns.items():\n",
    "    hi_text = hi_text.replace(hin_word, eng_word)\n",
    "\n",
    "print(hi_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\amrit\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT : So even if it's a big video, I will clearly mention all the products.\n",
      "तो यह एक बड़ा video है, तो भी मैं स्पष्ट रूप से सभी productsों का mention करेंगे।\n"
     ]
    }
   ],
   "source": [
    "# SENTENCE 2\n",
    "en_article = input(\"INPUT : \")\n",
    "nouns = spot_nouns_verbs(en_article)\n",
    "hi_text = hi_translation(en_article)\n",
    "# Loop through the dictionary and perform replacements\n",
    "for eng_word, hin_word in nouns.items():\n",
    "    hi_text = hi_text.replace(hin_word, eng_word)\n",
    "\n",
    "print(hi_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT : I was waiting for my bag.\n",
      "मैं अपने बैग के लिए wait कर रहा था.\n"
     ]
    }
   ],
   "source": [
    "# SENTENCE 3\n",
    "en_article = input(\"INPUT : \")\n",
    "nouns = spot_nouns_verbs(en_article)\n",
    "hi_text = hi_translation(en_article)\n",
    "\n",
    "# Loop through the dictionary and perform replacements\n",
    "for eng_word, hin_word in nouns.items():\n",
    "    hi_text = hi_text.replace(hin_word, eng_word)\n",
    "\n",
    "print(hi_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
